# Complete Playbook: Adding a RAG AI Chatbot to a Static Website

This is a detailed, proven playbook that was used to build and deploy a working AI chatbot on a live real estate website (https://420428stocker.laaa.com). The chatbot answers questions from source documents (PDFs, Word docs, spreadsheets, text files) using Claude AI. It was fully built and deployed by a Cursor AI agent.

Another Cursor AI agent should be able to follow this step-by-step to replicate the same system on any website.

---

## What This System Does

A user visits the website, clicks a chat bubble in the bottom-right corner, and can ask questions about the content. The system:

1. Embeds the user's question into a vector using Voyage AI
2. Searches a Pinecone vector database for the most relevant document chunks
3. Sends those chunks + the question to Claude, which generates a scoped answer
4. Streams the response back in real-time with source citations

All source documents (PDFs, DOCX, XLSX, TXT, MD files) are parsed, chunked, and embedded at build time. The chatbot is injected into the website HTML automatically.

---

## Architecture Overview

There are 4 services (all free tier except Claude which is pay-per-use):

- **Voyage AI** -- converts text into vector embeddings (free: 200M tokens)
- **Pinecone** -- stores and searches vectors (free: 100K vectors)
- **Cloudflare Workers** -- API proxy between browser and all APIs (free: 100K req/day)
- **Claude API (Anthropic)** -- generates chat responses (pay per use: ~$0.003/question with Haiku)

There are 3 code components:

- **`rag_pipeline.py`** (~1,300 lines) -- Python module that parses documents, chunks them, generates embeddings, uploads to Pinecone, and generates the chat widget HTML/CSS/JS
- **`laaa-chat-worker/src/index.js`** (~430 lines) -- Cloudflare Worker that handles CORS, embeds questions, searches Pinecone, streams Claude responses
- **Integration code in the build script** (~100 lines added) -- wires the RAG pipeline into the existing HTML build process

---

## Prerequisites

Before starting, the person needs:

- A static HTML website (can be generated by a Python build script or just a plain HTML file)
- A GitHub repository for the website, deployed via GitHub Pages
- A Cloudflare account (free, for the Worker)
- Python 3.10+ installed
- Node.js 18+ installed
- `gh` CLI installed and authenticated (`gh auth login`)
- Access to a terminal (PowerShell on Windows, bash on Mac/Linux)

---

## PHASE 1: Account Setup (15 minutes)

### 1A. Anthropic (Claude API)

- Go to https://console.anthropic.com
- Sign up or log in
- Go to Settings > API Keys, create a key
- Copy it (starts with `sk-ant-`)
- Add a payment method (required for API usage)

### 1B. Voyage AI

- Go to https://dash.voyageai.com
- Sign up
- Go to API Keys, create one (starts with `pa-`)
- **IMPORTANT:** Go to Billing and add a payment method. Even though 200M tokens are free, without a payment method you get crippling rate limits (3 RPM, 10K TPM) that will cause the build to fail or take hours. With payment method added, rate limits increase after several minutes.

### 1C. Pinecone

- Go to https://app.pinecone.io
- Sign up
- Create a new index with EXACTLY these settings:
  - **Name:** Choose a name (e.g., `my-chatbot`)
  - **Dimensions:** `1024` (MUST match Voyage AI's voyage-3 model output)
  - **Metric:** `cosine`
  - **Cloud:** AWS
  - **Region:** `us-east-1` (free tier)
- Copy the API key (starts with `pcsk_`)
- Copy the Index Host URL from the index detail page (looks like `https://my-chatbot-abc1234.svc.aped-4627-b74a.pinecone.io`)

### 1D. Cloudflare

- Go to https://dash.cloudflare.com
- Sign up or log in (free plan is fine)
- In a terminal, run `npx wrangler login` to authenticate (opens browser)

---

## PHASE 2: Create Project Files

All code files are available in the reference repo: https://github.com/gscher1311/420-428-stocker-bov

You can copy `rag_pipeline.py`, the `laaa-chat-worker/` folder, `requirements.txt`, and `.env.example` directly from that repo. Then customize for your site.

### 2A. Create `.env.example` and `.env`

Create `.env.example` in the repo root (committed to git as a template):

```
ANTHROPIC_API_KEY=sk-ant-REPLACE_ME
VOYAGE_API_KEY=pa-REPLACE_ME
PINECONE_API_KEY=pcsk_REPLACE_ME
PINECONE_INDEX_HOST=https://REPLACE_ME.svc.aped-XXXX.pinecone.io
```

Copy it to `.env` and fill in real keys. Each key prefix:

- Anthropic: `sk-ant-api03-...`
- Voyage: `pa-...`
- Pinecone: `pcsk_...`
- Pinecone Host: `https://index-name-xxxxxxx.svc.aped-xxxx.pinecone.io`

**GOTCHA:** Users tend to double the prefix (e.g., `sk-ant-sk-ant-...`) because the template already has `sk-ant-` and they paste the full key. Check that each key has the prefix exactly ONCE.

**GOTCHA:** Users tend to paste the Pinecone host URL into the template without removing the template suffix, creating something like `https://real-url.pinecone.io.svc.aped-XXXX.pinecone.io`. The correct value is JUST the URL from the Pinecone dashboard, like `https://my-chatbot-abc1234.svc.aped-4627-b74a.pinecone.io`.

### 2B. Update `.gitignore`

Add these lines to `.gitignore`:

```
__pycache__/
.env
laaa-chat-worker/node_modules/
laaa-chat-worker/.wrangler/
```

### 2C. Create `requirements.txt`

```
pymupdf>=1.24.0
python-docx>=1.1.0
openpyxl>=3.1.0
voyageai>=0.3.0
pinecone>=5.0.0
tiktoken>=0.7.0
python-dotenv>=1.0.0
```

Install with: `pip install -r requirements.txt`

### 2D. Copy `rag_pipeline.py`

Copy directly from https://github.com/gscher1311/420-428-stocker-bov/blob/main/rag_pipeline.py

This file is generic and works on any website. It contains:

**Document parsers:**
- `parse_pdf(filepath)` -- uses PyMuPDF to extract text page-by-page. Skips pages with fewer than 30 chars (likely scanned images).
- `parse_docx(filepath)` -- extracts paragraphs and tables from Word docs.
- `parse_spreadsheet(filepath)` -- reads Excel files (.xlsx, .xlsm) with `data_only=True` to get cached cell values.
- `parse_text(filepath)` -- reads .txt and .md files directly.
- `capture_build_context(build_data)` -- takes a dict of structured data from your build script's Python variables and creates Document objects.
- `parse_all_documents(docs_dir, build_data)` -- master function that iterates all files in `docs/` and dispatches to the correct parser.

**Chunking engine:**
- `smart_chunk(documents)` -- splits documents into embedding-ready pieces.
- Document-type-aware: 500 tokens for narrative text, 900 tokens for tables/financial data.
- 50-token overlap between chunks for continuity.
- Respects sentence boundaries (never splits mid-sentence for narrative text).

**Embedding + upload:**
- `embed_chunks(chunks, namespace)` -- generates embeddings via Voyage AI `voyage-3` model (1024 dims).
- Batch size of 8 with 21-second pauses to handle free-tier rate limits.
- Retry logic with exponential backoff for rate limit errors.
- `upload_vectors(vectors, namespace)` -- uploads to Pinecone under a property-specific namespace.

**Chat widget generator:**
- `generate_chat_widget(...)` -- returns self-contained HTML/CSS/JS (~550 lines) for the chat UI.
- Features: floating bubble, slide-up panel, starter question pills, markdown rendering, source citations, typing indicator, disclaimer footer, 30-message session limit, mobile responsive, hidden in print/PDF.

**Pipeline runner:**
- `run_rag_pipeline(docs_dir, namespace, build_data)` -- orchestrates: parse -> chunk -> embed -> upload.

### 2E. Create the Cloudflare Worker

Copy the `laaa-chat-worker/` folder from the reference repo, or create from scratch:

**Directory structure:**
```
laaa-chat-worker/
  package.json
  wrangler.toml
  src/
    index.js
```

**`package.json`:**
```json
{
  "name": "laaa-chat-worker",
  "version": "1.0.0",
  "description": "Cloudflare Worker proxy for AI chatbot",
  "main": "src/index.js",
  "scripts": { "dev": "wrangler dev", "deploy": "wrangler deploy" },
  "devDependencies": { "wrangler": "^3.0.0" }
}
```

**`wrangler.toml`:**
```toml
name = "laaa-chat-worker"
main = "src/index.js"
compatibility_date = "2024-12-01"
```

**`src/index.js`** -- copy from https://github.com/gscher1311/420-428-stocker-bov/blob/main/laaa-chat-worker/src/index.js

**Things to customize in `index.js`:**

1. **`CLAUDE_MODEL`** (line 21) -- set to `"claude-3-haiku-20240307"` (verified working Feb 2026). **GOTCHA:** `claude-3-5-haiku-20241022` was deprecated and returns 404. Always test the model name.
2. **`SYSTEM_PROMPT`** -- customize the persona for your friend's use case (currently set for real estate BOV assistant).
3. **CORS origins in `getCorsHeaders()`** -- change `origin.endsWith(".laaa.com")` to match your friend's domain (e.g., `origin.endsWith(".mydomain.com")`). Also allows localhost for testing.

### 2F. Modify the Website Build Script

The build script (whatever generates the HTML) needs these additions:

**At the top -- add config variables:**

```python
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# RAG Chatbot Config
ENABLE_CHATBOT = True  # Set False to build without chatbot
BOV_NAMESPACE = "my-property-123"  # unique per property/site
CHAT_WORKER_URL = "https://your-worker-name.your-account.workers.dev"
PROPERTY_DISPLAY_NAME = "123 Main St"
STARTER_QUESTIONS = [
    "What is the asking price?",
    "Summarize the key details",
    "What are the financials?",
    "Tell me about the location"
]
```

**At the end -- BEFORE `</body></html>`, AFTER all other HTML/JS:**

**CRITICAL:** The closing `</body></html>` must be separated from the main HTML so the chat widget can be injected between the page content and the closing tags. If your build script has `</body></html>` embedded inside a string concatenation or template, split it out.

```python
# === RAG CHATBOT: BUILD KNOWLEDGE BASE + INJECT CHAT WIDGET ===
if ENABLE_CHATBOT:
    try:
        from rag_pipeline import (
            run_rag_pipeline, generate_chat_widget, capture_build_context
        )

        # Structured data from your build script's Python variables.
        # This is BETTER than parsing your own HTML output.
        build_data = {
            "property_name": "123 Main St, City, ST 12345",
            "list_price": LIST_PRICE,  # or whatever your variable is
            "units": UNITS,
            "sf": SF,
            "rent_roll": RENT_ROLL,    # list of tuples
            "sale_comps": SALE_COMPS,   # list of dicts
            "financial_summary": "formatted string of key financial metrics",
            "operating_statement": "formatted string of operating statement",
            "sections": {},  # optional: named text sections
        }

        docs_dir = os.path.join(SCRIPT_DIR, "docs")
        chunks, vectors = run_rag_pipeline(
            docs_dir=docs_dir,
            namespace=BOV_NAMESPACE,
            build_data=build_data,
            verbose=True,
        )

        chat_html = generate_chat_widget(
            worker_url=CHAT_WORKER_URL,
            namespace=BOV_NAMESPACE,
            property_name=PROPERTY_DISPLAY_NAME,
            starter_questions=STARTER_QUESTIONS,
        )
        html_parts.append(chat_html)
        print(f"Chat widget injected for namespace: {BOV_NAMESPACE}")

    except ImportError as e:
        print(f"\nWARNING: RAG dependencies not installed ({e}).")
        print("Building without chatbot. Install with: pip install -r requirements.txt")
    except Exception as e:
        print(f"\nWARNING: RAG pipeline failed ({e}).")
        print("Building without chatbot. Check your .env API keys and try again.")

# Close the HTML document
html_parts.append("</body></html>")

# Write output
html = "".join(html_parts)
with open(OUTPUT, "w", encoding="utf-8") as f:
    f.write(html)
```

**KEY DESIGN DECISIONS:**
- The `from rag_pipeline import` is INSIDE `if ENABLE_CHATBOT:` AND inside `try/except`. The build script will NOT crash if RAG deps aren't installed or APIs fail. It gracefully builds without the chatbot.
- The `build_data` dict captures data from Python variables, not from parsing the HTML.
- `</body></html>` is appended AFTER the chat widget.

---

## PHASE 3: Install Dependencies (5 minutes)

```bash
pip install -r requirements.txt
cd laaa-chat-worker
npm install
cd ..
```

---

## PHASE 4: Deploy the Cloudflare Worker (5 minutes)

```bash
cd laaa-chat-worker
npx wrangler deploy
```

Note the deployed URL (e.g., `https://your-worker.your-account.workers.dev`). Update `CHAT_WORKER_URL` in your build script to match.

Set the 4 API secrets. You can pipe values to avoid interactive prompts:

```bash
echo "sk-ant-api03-YOUR_FULL_KEY" | npx wrangler secret put ANTHROPIC_API_KEY
echo "pa-YOUR_FULL_KEY" | npx wrangler secret put VOYAGE_API_KEY
echo "pcsk_YOUR_FULL_KEY" | npx wrangler secret put PINECONE_API_KEY
echo "https://your-index.svc.aped-xxxx.pinecone.io" | npx wrangler secret put PINECONE_INDEX_HOST
```

Verify all 4 secrets are set: `npx wrangler secret list`

**GOTCHA:** If `npx wrangler whoami` says "not authenticated", run `npx wrangler login` first.

---

## PHASE 5: Add Source Documents

Create a `docs/` folder in the repo root and drop in ALL source documents:

- PDFs (financials, reports, records)
- Word docs (.docx)
- Excel files (.xlsx, .xlsm)
- Text files (.txt)
- Markdown files (.md)
- Images are SKIPPED by the parser (Phase 2 enhancement with Claude Vision)

The more documents you provide, the smarter the chatbot. The Stocker BOV had 22 source documents producing 476 vectors.

---

## PHASE 6: Run the Build (~20 minutes first time)

```bash
python build_bov_v2.py
```

Expected output:
```
============================================================
BUILDING RAG KNOWLEDGE BASE
============================================================

Step 1: Parsing documents...
  Parsed file1.pdf: 6 segment(s)
  Parsed file2.xlsx: 32 segment(s)
  ...
  Total: 141 document segments parsed

Step 2: Chunking documents...
  Total: 476 chunks created
  Tabular chunks: 422, Narrative chunks: 54

Step 3: Generating embeddings (Voyage AI)...
  60 batches of 8 (est. 21 min with rate limit pauses)
  Embedding batch 1/60 (8 chunks)... OK
  Embedding batch 2/60 (8 chunks)... OK
  ...

Step 4: Uploading to Pinecone...
  Uploaded batch 1/5
  ...
  Total vectors in namespace 'my-property-123': 476

Chat widget injected for namespace: my-property-123

index.html generated.
Done!
```

**GOTCHA:** First build takes ~20 minutes due to Voyage AI rate limit pauses (21s between batches). This is normal.

**GOTCHA:** If you see "RAG pipeline failed" with a message about "payment method", go to Voyage AI billing, add a payment method, wait 5-10 minutes, then retry.

---

## PHASE 7: Push to GitHub and Go Live

```bash
git add -A
git commit -m "Add RAG AI chatbot"
git push
```

Visit the live site. Click the blue chat bubble in the bottom-right corner.

---

## PHASE 8: Verify It Works

Test these scenarios:

1. **Factual question** (e.g., "What's the asking price?") -- should answer accurately
2. **Comparative question** (e.g., "How do the comps compare?") -- should synthesize data
3. **Off-topic question** (e.g., "What's the weather?") -- should refuse politely
4. **Check source citations** appear under each answer (click "Sources" to expand)
5. **Check disclaimer** appears under each answer
6. **Check mobile** -- chat panel should go full-screen on small screens
7. **Check print/PDF** -- chatbot should be completely hidden

### If you get "Sorry, I encountered an error"

This means the Worker is reachable but something failed internally. Debug in order:

**Step 1: Check the Claude model name.**
The most common issue. Test with curl:
```bash
curl -X POST https://api.anthropic.com/v1/messages \
  -H "x-api-key: YOUR_ANTHROPIC_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "Content-Type: application/json" \
  -d '{"model":"claude-3-haiku-20240307","max_tokens":10,"messages":[{"role":"user","content":"Hi"}]}'
```
If you get a 404, the model name is wrong. Find a working one and update `CLAUDE_MODEL` in `index.js`, then redeploy: `npx wrangler deploy`.

**Step 2: Verify Worker secrets.**
```bash
npx wrangler secret list
```
Must show all 4: `ANTHROPIC_API_KEY`, `VOYAGE_API_KEY`, `PINECONE_API_KEY`, `PINECONE_INDEX_HOST`.

**Step 3: Test each API individually.**
Write a Python script that tests Voyage embed, Pinecone query, and Claude call separately (as we did during debugging).

---

## Key Gotchas We Encountered (Save Hours of Debugging)

1. **Claude model `claude-3-5-haiku-20241022` was deprecated.** Returns 404. Use `claude-3-haiku-20240307` or `claude-sonnet-4-20250514`. Always verify the model name works before deploying.

2. **Voyage AI rate limits without payment method.** Even though 200M tokens are free, without a payment method you get 3 RPM / 10K TPM. The build sends 60+ embedding requests. Add payment method, wait several minutes for limits to update.

3. **Doubled API key prefixes.** When pasting keys into a template that has `sk-ant-REPLACE_ME`, users get `sk-ant-sk-ant-...`. Always check each key has its prefix exactly ONCE.

4. **Pinecone host URL mangled.** Users paste the real URL on top of the template placeholder, getting `https://real.pinecone.io.svc.aped-XXXX.pinecone.io`. Correct value has NO template text.

5. **`</body></html>` injection point.** The chat widget HTML must go BEFORE `</body></html>`. If closing tags are embedded in a string concatenation, split them into a separate append.

6. **Cloudflare Workers are stateless.** In-memory rate limiting counters reset every request. Use Cloudflare dashboard WAF rules for rate limiting.

7. **CORS wildcard for multiple sites.** Don't hardcode origins. Use `origin.endsWith('.yourdomain.com')` so new sites work without Worker redeployment.

8. **Conditional imports prevent crashes.** `from rag_pipeline import ...` must be inside `if ENABLE_CHATBOT:` and inside `try/except`. Otherwise build crashes when RAG deps aren't installed.

9. **openpyxl warnings are normal.** "Conditional Formatting extension is not supported" is harmless.

10. **Pinecone dimensions must match.** Index MUST be 1024 dimensions to match voyage-3 output. Mismatched dims cause silent failures.

11. **Scanned PDFs return empty text.** PDFs that are scanned images (not searchable text) will produce no content without Tesseract OCR. Workaround: manually copy key info to a .txt file in `docs/`.

12. **Excel macro values may be stale.** `.xlsm` files with VBA macros: openpyxl reads cached values only. Open in Excel, let macros run, save before adding to `docs/`.

---

## Costs

| Service | Monthly Cost | Per-Question Cost | Notes |
|---------|-------------|-------------------|-------|
| Voyage AI | $0 | ~$0.0001 | Free: 200M tokens/mo |
| Pinecone | $0 | $0 | Free: 100K vectors |
| Cloudflare Worker | $0 | $0 | Free: 100K req/day |
| Claude Haiku | Pay-per-use | ~$0.003 | Fast, cheap |
| Claude Sonnet | Pay-per-use | ~$0.015 | Smarter, pricier |
| Per-site build | One-time | ~$0.001 | Embedding cost |

Realistic monthly cost with 10 active sites, 50 questions each: **$1.50-$7.50/mo total.**

---

## Reference: The Live Working Example

- **Website:** https://420428stocker.laaa.com (click the blue chat bubble)
- **GitHub repo:** https://github.com/gscher1311/420-428-stocker-bov
- **Worker URL:** https://laaa-chat-worker.laaa-team.workers.dev
- **Pinecone namespace:** `stocker-420`
- **Knowledge base:** 476 vectors from 22 source documents (141 parsed segments, 476 chunks)
- **Build time:** ~22 minutes (with Voyage rate limit pauses)
- **Files:** `rag_pipeline.py` (1,301 lines), `index.js` (434 lines), plus ~100 lines added to build script

---

## For Multiple Sites / Properties

The system is designed for multi-tenant use:

- **One Cloudflare Worker** serves all sites (routes by namespace in the request body)
- **One Pinecone index** stores all sites (each site gets its own namespace, e.g., `property-123`, `property-456`)
- **One set of API keys** works across all sites (same `.env`)
- **Per-site config:** Change `BOV_NAMESPACE`, `PROPERTY_DISPLAY_NAME`, `STARTER_QUESTIONS`, and `build_data` in the build script
- **Per-site docs:** Each site has its own `docs/` folder with relevant source documents

Adding a new site: clone the template, fill in config, add docs, run build, push. The Worker and Pinecone index don't need any changes.
